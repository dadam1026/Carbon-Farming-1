{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Example Kaggle notebook: https://colab.research.google.com/github/karthikraja95/fsdl_deforestation_detection/blob/master/fsdl_deforestation_detection/experimental/FSDL_Final_Model.ipynb#scrollTo=Fq9IBjOtUupg"
      ],
      "metadata": {
        "id": "8jcxIXO1oT2p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UUJ3n7gtalH",
        "outputId": "02590bce-6a1f-4b59-a92d-270109bcdbbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n"
          ]
        }
      ],
      "source": [
        "! pip install torch torchvision torchaudio\n",
        "! pip install -Uq kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchsummary import summary\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "SjoI17gjpXRP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Dataset from Kaggle\n",
        "\n",
        "1.  First set up the Kaggle API and download kaggle.json using these instructions: https://github.com/Kaggle/kaggle-api\n",
        "2.   Run the following cell and upload kaggle.json\n"
      ],
      "metadata": {
        "id": "R279HpQCuNUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files \n",
        "files.upload()"
      ],
      "metadata": {
        "id": "n0ds_arguHmO",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "outputId": "00928951-c3db-4db9-e7c4-382e7195a895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-80f35d64-0bac-47d2-b5f2-75047280bfe4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-80f35d64-0bac-47d2-b5f2-75047280bfe4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "NW2kTRDmuVWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download nikitarom/planets-dataset"
      ],
      "metadata": {
        "id": "8iRruTQKvigW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip planets-dataset.zip"
      ],
      "metadata": {
        "id": "DGHUvvDXvxaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Dataset"
      ],
      "metadata": {
        "id": "RyCeNrJBzvvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = Path('./planet/planet')\n",
        "train_df = pd.read_csv(path/'train_classes.csv')\n",
        "train_df"
      ],
      "metadata": {
        "id": "CQyefWxbwwvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data for modeling"
      ],
      "metadata": {
        "id": "35BPcsyGBtqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know there are 17 total labels. Select the labels we think are most responsible for deforestation - agriculture, slash-burn, habitation, selective-logging, artisinal_mine, conventional_mine, cultivation"
      ],
      "metadata": {
        "id": "WMeHxI1c6Hgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deforestation_labels = ['agriculture', 'slash-burn', 'habitation', 'selective-logging',  'artisinal_mine', 'conventional_mine', 'cultivation']"
      ],
      "metadata": {
        "id": "x4PDnB4dBLTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we're focusing on deforestation/green, select conserved labels as well. "
      ],
      "metadata": {
        "id": "qIEtC1XuBnGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conserved_labels = ['primary']"
      ],
      "metadata": {
        "id": "tewwwTP0CDoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine these labels to create the binary class 'deforestation' - 0 for conserved, 1 if conservation is detected.\n",
        "\n",
        "#### If any of the deforestation labels are found in an image's tags, deforestation = 1. Otherwise, deforestation = 0."
      ],
      "metadata": {
        "id": "bRppvh0FRklm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['deforestation'] = 0\n",
        "\n",
        "tag = train_df['tags']\n",
        "print(type(tag[0]))\n",
        "\n",
        "deforested = tag.str.contains('|'.join(deforestation_labels))\n",
        "train_df['deforestation'] = deforested.astype(int)\n",
        "train_df\n",
        "\n",
        "\n",
        "# for label in deforestation_labels:\n",
        "#   for i in range(len(train_df)):\n",
        "#     if train_df['deforestation'].iloc[i] == 1:\n",
        "#       continue\n",
        "#     if label in train_df['tags'].iloc[i].split():\n",
        "#       train_df['deforestation'].iloc[i] = 1\n",
        "     \n",
        "display(train_df.head())\n",
        "\n",
        "#if tags contains any label from deforestation_labels, it has a postive value of deforestation (1)"
      ],
      "metadata": {
        "id": "yMqc9GQ0FZw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store images in a PyTorch Dataset\n"
      ],
      "metadata": {
        "id": "OWTt2Tbi0IrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir /content/planet/planet/data\n",
        "! mv /content/planet/planet/train-jpg /content/planet/planet/data/train-jpg\n",
        "! mkdir /content/planet/planet/validation\n",
        "! mv /content/planet/planet/test-jpg /content/planet/planet/validation/test-jpg"
      ],
      "metadata": {
        "id": "Y9TA3MrA8vGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are using a binary classification model, so our image classes are 1 - significant deforestation; 0 - no significant deforestation\n",
        "#Setting up transforms\n",
        "data_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize([0.485,0.456,0.406], [0.225, 0.225, 0.225])])\n",
        "\n",
        "#Set up dataset for training set and validation set \n",
        "train_data = datasets.ImageFolder('/content/planet/planet/data/', transform = data_transform)\n",
        "val_data = datasets.ImageFolder('/content/planet/planet/validation', transform=data_transform)\n",
        "\n",
        "#check to make sure number of images and df rows is the same\n",
        "print('Training data images: ', len(train_data))\n",
        "print('Training data csv: ', len(train_df))\n",
        "print('Validation data images: ', len(val_data))\n",
        "\n",
        "print(train_data)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(0)"
      ],
      "metadata": {
        "id": "Mj936HBU5vXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up dataloaders"
      ],
      "metadata": {
        "id": "esf5TXefQhWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "images, labels = iter(train_loader).next()\n",
        "print(images.shape)\n",
        "images = images.numpy()\n"
      ],
      "metadata": {
        "id": "gYIU2pKmQuL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the images in the batch"
      ],
      "metadata": {
        "id": "jBflMBoPXXN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(15, 5))\n",
        "for idx in np.arange(batch_size):\n",
        "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
        "    image = images[idx]\n",
        "    image = image.transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.225, 0.225, 0.225])\n",
        "    image = std * image + mean\n",
        "    image = np.clip(image, 0, 1)\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(\"{}\".format(train_df['deforestation'].iloc[idx]))"
      ],
      "metadata": {
        "id": "yihNhLbNXVeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "M8Wv81beV7KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    #Input shape: (batch_size,3,256,256)\n",
        "        \n",
        "    # Convolutional 1 layer: 3x3 kernel, stride=1, padding=0, 2 output channels / feature maps\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=0)\n",
        "    # Conv1 layer output size = (W-F+2P)/S+1 = ((256-3)/1)+1 = 254\n",
        "    # Conv1 layer output shape for one image: [2,254,254]\n",
        "    \n",
        "    # Maxpool layer: kernel_size=2, stride=2\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    # Pool output shape for one image: ((254-2)/2)+1 = 127 [2,127,127]\n",
        "    \n",
        "    # Convolutional 2 layer: 3x3 kernel, stride=1, padding=0, 20 output channels / feature maps\n",
        "    self.conv2 = nn.Conv2d(in_channels=2,out_channels=4,kernel_size=3, stride=1, padding=0)\n",
        "    # Conv2 layer output size = (W-F+2P)/S+1 = ((127-3)/1)+1 = 125\n",
        "    # Conv2 layer output shape for one image: [4,125,125]\n",
        "    \n",
        "    # Maxpool layer: kernel_size=2, stride=2\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    # Pool output shape for one image: ((125-2)/2)+1 = 62 (rounded down)  [4,62,62]\n",
        "    \n",
        "    # Input size: 4 * 62 * 62 = 15376  from pool2 pooling layer\n",
        "    # 2 output channels (for the 2 classes)\n",
        "    self.fc1 = nn.Linear(4*62*62, 2)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # Two convolutional layers followed by relu and then pooling\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool1(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    # Flatten into a vector to feed into linear layer\n",
        "    x = x.view(x.size(0), -1)\n",
        "    \n",
        "    # Linear layer\n",
        "    x = self.fc1(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "7VzJrnBpV-HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = ConvNet()\n",
        "\n",
        "#display a summary of the layers of the model and output shape after each layer\n",
        "summary(net, (images.shape[1:]), batch_size=batch_size, device='cpu')"
      ],
      "metadata": {
        "id": "Y5i1kgvqi4Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a cost/loss function and optimizer"
      ],
      "metadata": {
        "id": "fyOgo70jk-9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "nSYgyU4TlCpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "01G9ESBAlL2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, n_epochs, device):\n",
        "  loss_over_time = [] # to track the loss as the network trains \n",
        "  model = model.to(device) # Send model to GPU if available\n",
        "  model.train() # Set the model to training mode\n",
        "\n",
        "  for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "      \n",
        "      running_loss = 0.0\n",
        "      \n",
        "      for i, data in enumerate(train_loader):\n",
        "          \n",
        "          # Get the input images and labels, and send to GPU if available\n",
        "          inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "          # Zero the weight gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass to get outputs\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          # Calculate the loss\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          # Backpropagation to get the gradients with respect to each weight\n",
        "          loss.backward()\n",
        "\n",
        "          # Update the weights\n",
        "          optimizer.step()\n",
        "\n",
        "          # Convert loss into a scalar and add it to running_loss\n",
        "          running_loss += loss.item()\n",
        "          \n",
        "          if i % 1000 == 999:    # print every 1000 batches\n",
        "              avg_loss = running_loss/1000\n",
        "              # record and print the avg loss over the 1000 batches\n",
        "              loss_over_time.append(avg_loss)\n",
        "              print('Epoch: {}, Batch: {}, Avg. Loss: {:.4f}'.format(epoch + 1, i+1, avg_loss))\n",
        "              running_loss = 0.0\n",
        "\n",
        "  return loss_over_time"
      ],
      "metadata": {
        "id": "PO2e80D9lLHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_epochs = 5\n",
        "cost_path = train_model(net, criterion, optimizer, train_loader, n_epochs, device)\n",
        "\n",
        "# visualize the loss\n",
        "plt.plot(cost_path)\n",
        "plt.xlabel('Batch (1000s)')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mCIEJDqxl5v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Validate the model on the validation set\n",
        "\n",
        "def test_model(model,test_loader,device):\n",
        "    # Turn autograd off\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set the model to evaluation mode\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        # Set up lists to store true and predicted values\n",
        "        y_true = []\n",
        "        test_preds = []\n",
        "        test_probs = []\n",
        "\n",
        "        # Calculate the predictions on the test set and add to list\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            # Feed inputs through model to get raw scores\n",
        "            logits = model.forward(inputs)\n",
        "            # Convert raw scores to probabilities \n",
        "            probs = F.softmax(logits,dim=1)\n",
        "            # Get discrete predictions using argmax\n",
        "            preds = np.argmax(probs.cpu().numpy(),axis=1)\n",
        "            # Add predictions and actuals to lists\n",
        "            test_preds.extend(preds)\n",
        "            test_probs.extend(probs)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate the accuracy\n",
        "        test_preds = np.array(test_preds)\n",
        "        test_probs = np.array(test_probs)\n",
        "        y_true = np.array(y_true)\n",
        "        test_acc = np.sum(test_preds == y_true)/y_true.shape[0]\n",
        "        \n",
        "        # Recall for each class\n",
        "        recall_vals = []\n",
        "        for i in range(10):\n",
        "            class_idx = np.argwhere(y_true==i)\n",
        "            total = len(class_idx)\n",
        "            correct = np.sum(test_preds[class_idx]==i)\n",
        "            recall = correct / total\n",
        "            recall_vals.append(recall)\n",
        "    \n",
        "    return test_acc, recall_vals,test_preds,test_probs"
      ],
      "metadata": {
        "id": "rYcyJgpQpkLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the test set accuracy and recall for each class\n",
        "acc,recall_vals, preds, probs = test_model(net,val_loader,\"cpu\")\n",
        "print('Validation set accuracy is {:.3f}'.format(acc))\n",
        "\n",
        "print(preds)\n",
        "print(probs)"
      ],
      "metadata": {
        "id": "QhuQAY6JqLVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a batch of predictions\n",
        "\n",
        "with torch.no_grad():\n",
        "    net = net.to(device)\n",
        "    net.eval()\n",
        "    # Get a batch of test images\n",
        "    dataiter = iter(val_loader)\n",
        "    images, labels = dataiter.next()\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    # get predictions\n",
        "    preds = np.squeeze(net(images).max(1, keepdim=True)[1].cpu().numpy())\n",
        "    images = images.cpu().numpy()\n",
        "\n",
        "# Plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(batch_size):\n",
        "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    ax.set_title(\"{} ({})\".format(preds[idx], classes[labels[idx]]),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
      ],
      "metadata": {
        "id": "2-oeVCQlyBuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ghp2oe_TzHsg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}